{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating positive connection strengths\n",
    "The values in each subject's connectivity matrix can be either positive or negative. Positive values mean the two areas are likely to be active at the same time, while a negative value means the areas are likely to not be active at the same time. A value near zero means there is not much correlation between when the areas are activated.\n",
    "\n",
    "Previously I considered both positive and negative connections for predicting ADHD, but here I will try using only positive connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score, roc_auc_score, plot_roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, ADASYN\n",
    "\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get metadata for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cm_table.html\", \"r\") as f:\n",
    "    table = f.read()\n",
    "\n",
    "soup = BeautifulSoup(table, \"html.parser\")\n",
    "\n",
    "rows = soup.find_all(class_=\"powerTable\")[1].tbody.find_all(\"tr\")[3:523]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = defaultdict(list)\n",
    "for row in rows:\n",
    "    text_list = list(row.stripped_strings)\n",
    "    if len(text_list) == 13:\n",
    "        text_list.insert(7, 'na') # insert so list is standard size when that column was empty on the webpage\n",
    "    cols[\"study\"].append(text_list[2])\n",
    "    cols[\"id\"].append(text_list[3].replace(\"_\", \"\"))\n",
    "    cols[\"age\"].append(float(text_list[8]))\n",
    "    cols[\"gender\"].append(text_list[10])\n",
    "    cols[\"label\"].append(text_list[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.DataFrame(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the connectivity matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conn_matrices():\n",
    "    file_names = os.listdir(\"data/ADHD200_CC200\")\n",
    "\n",
    "    cm_file_re = r\"^\\S+connectivity_matrix_file\\.txt$\"\n",
    "\n",
    "    conn_matrices = OrderedDict()\n",
    "    for file_name in file_names:\n",
    "        if re.match(cm_file_re, file_name):\n",
    "            id_ = \"\".join(file_name.split(\"_\")[:-3])\n",
    "        \n",
    "            cm = np.empty((190,190))\n",
    "            with open(\"data/ADHD200_CC200/{}\".format(file_name)) as f:\n",
    "                for idx, row in enumerate(f):\n",
    "                    row = row.strip().split(\" \")\n",
    "                    row = list(map(np.float, row))\n",
    "                    cm[idx, :] = row\n",
    "            \n",
    "            less_than_zero_mask = cm < 0 # get indices of values less than zero\n",
    "            cm[less_than_zero_mask] = 0 # set values less than zero to zero\n",
    "            \n",
    "            conn_matrices[id_] = cm\n",
    "    return conn_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regions():\n",
    "    \"\"\"\n",
    "    Gets the names of the regions (in order of appearance in connectivity matrix). \n",
    "    All files have the same order of regions, so we only to need to get this once.\n",
    "    Some region names are repeated because there are multiple points within that region,\n",
    "        so numbers are appended to the region names to distinguish them.\n",
    "    \n",
    "    returns a list of strings\n",
    "    \"\"\"\n",
    "    regions_path = \"data/ADHD200_CC200/KKI_1018959_region_names_full_file.txt\"\n",
    "    regions = []\n",
    "    with open(regions_path, \"r\") as f:\n",
    "        regions = [region.strip().replace(\" \", \"_\") for region in f]\n",
    "    names = defaultdict(int)\n",
    "    distinct_region_names = []\n",
    "    for region in regions:\n",
    "        distinct_region_names.append(region+\"_\"+str(names[region]))\n",
    "        names[region] += 1\n",
    "    return distinct_region_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_names = get_regions()\n",
    "\n",
    "conn_matrices = get_conn_matrices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_conn_matrices(conn_matrices_dict, region_names):\n",
    "    \"\"\"\n",
    "    Flatten a cm dictionary (mapping subjects to connectivity matrices), such that each unique value in the\n",
    "        connectivity matrix is a column feature in a row.\n",
    "    Returns: 1) a numpy array where each row represents a subject with each column a feature;\n",
    "             2) a list of the subject ids in the order they appear in the feature array;\n",
    "             3) a list of the feature names in the order they appear in the feature array.\n",
    "    The subjects list holds the row labels, feature_names list holds column labels.\n",
    "    \"\"\"\n",
    "    subjects = list(conn_matrices_dict.keys())\n",
    "    num_rows = len(subjects)\n",
    "    features = np.empty((num_rows, 17955))\n",
    "    \n",
    "    # adjacency matrices have duplicate values, only need values from half of the matrix (and don't need diagonal)\n",
    "    # np.tril_indices() returns indices of unique values\n",
    "    row_idxs, col_idxs = np.tril_indices(190, k=-1)\n",
    "    for idx, subject in enumerate(subjects):\n",
    "        cm = conn_matrices_dict[subject]\n",
    "        row = np.array([cm[row_idx, col_idx] for row_idx, col_idx in zip(row_idxs, col_idxs)])\n",
    "        features[idx, :] = row\n",
    "    \n",
    "    feature_names = [region_names[row_idx]+\"_to_\"+region_names[col_idx] \n",
    "                     for row_idx, col_idx in zip(row_idxs, col_idxs)]\n",
    "    \n",
    "    return features, subjects, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, subjects, feature_names = flatten_conn_matrices(conn_matrices, region_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up some memory\n",
    "del conn_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_metadata(metadata, subjects):\n",
    "    \"\"\"\n",
    "    Sorts a metadata dataframe so that the order is the same as the order of subjects in the subjects list.\n",
    "    :arg metadata: dataframe with ADHD200 metadata\n",
    "    :arg subjects: a list of subjects of specific order\n",
    "    \"\"\"\n",
    "    metadata_ids = metadata[\"id\"].values\n",
    "    subjects_order_in_metadata = [np.where(metadata_ids==subject)[0][0] for subject in subjects]\n",
    "    metadata_subject_sort = metadata.iloc[subjects_order_in_metadata, :]\n",
    "    return metadata_subject_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the metadata so that the order is the same as in the feature matrix\n",
    "metadata_sorted = sort_metadata(metadata, subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add one hot vectors for each of the ADHD labels\n",
    "adhd = [0 if label == \"Typically Developing\" else 1 for label in metadata_sorted[\"label\"]]\n",
    "metadata_sorted = metadata_sorted.assign(adhd=adhd)\n",
    "metadata_sorted.drop(columns=\"label\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study</th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>adhd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>ADHD200_CC200</td>\n",
       "      <td>Peking23446674</td>\n",
       "      <td>14.58</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>ADHD200_CC200</td>\n",
       "      <td>NYU2054438</td>\n",
       "      <td>8.11</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>ADHD200_CC200</td>\n",
       "      <td>KKI2018106</td>\n",
       "      <td>11.66</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>ADHD200_CC200</td>\n",
       "      <td>Pittsburgh0016067</td>\n",
       "      <td>17.91</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>ADHD200_CC200</td>\n",
       "      <td>Peking21916266</td>\n",
       "      <td>13.17</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>ADHD200_CC200</td>\n",
       "      <td>NYU2983819</td>\n",
       "      <td>12.28</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>ADHD200_CC200</td>\n",
       "      <td>NYU0010028</td>\n",
       "      <td>9.42</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>ADHD200_CC200</td>\n",
       "      <td>NYU0010093</td>\n",
       "      <td>15.21</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>ADHD200_CC200</td>\n",
       "      <td>Peking27407032</td>\n",
       "      <td>13.42</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>ADHD200_CC200</td>\n",
       "      <td>Peking13086074</td>\n",
       "      <td>11.50</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>520 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             study                 id    age  gender  adhd\n",
       "442  ADHD200_CC200     Peking23446674  14.58    Male     1\n",
       "143  ADHD200_CC200         NYU2054438   8.11    Male     1\n",
       "47   ADHD200_CC200         KKI2018106  11.66    Male     0\n",
       "477  ADHD200_CC200  Pittsburgh0016067  17.91  Female     0\n",
       "395  ADHD200_CC200     Peking21916266  13.17    Male     0\n",
       "..             ...                ...    ...     ...   ...\n",
       "160  ADHD200_CC200         NYU2983819  12.28  Female     1\n",
       "211  ADHD200_CC200         NYU0010028   9.42    Male     1\n",
       "250  ADHD200_CC200         NYU0010093  15.21  Female     0\n",
       "399  ADHD200_CC200     Peking27407032  13.42    Male     0\n",
       "344  ADHD200_CC200     Peking13086074  11.50    Male     0\n",
       "\n",
       "[520 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_correlated_features(features, metadata, feature_names, p_val=.01):\n",
    "    \"\"\"\n",
    "    returns a DataFrame with a subset of the features which have a correlation p value less than the specified cutoff\n",
    "    :arg features: numpy feature matrix, sorted in the same order as the metadata.\n",
    "    :arg target: DataFrame with target and ids, sorted in the same order as the feature matrix.\n",
    "    :arg feature_names: the names of the features in the feature matrix, same order.\n",
    "    :arg p_val: the maximum p value for a feature to be included.\n",
    "    \"\"\"\n",
    "    # get the p values for correlations. lower is better!\n",
    "    target=metadata[\"adhd\"]\n",
    "    correlation_p_vals = np.array([pearsonr(features[:,col], target)[1] for col in range(features.shape[1])])\n",
    "    # get the order of columns which are most correlated with having adhd\n",
    "    corr_p_vals_argsort = correlation_p_vals.argsort()\n",
    "    # the number of features with correlation p values less than the cutoff\n",
    "    num_features = np.count_nonzero(correlation_p_vals < p_val)\n",
    "    # get the indices of features of features with p vals less than the cutoff\n",
    "    most_correlated = corr_p_vals_argsort[:num_features]\n",
    "    \n",
    "    features_most_correlated = features[:, most_correlated]\n",
    "    feature_names_most_correlated = [feature_names[idx] for idx in most_correlated]\n",
    "    \n",
    "    # make features dataframe with the smaller features\n",
    "    X = pd.DataFrame(features_most_correlated, columns=feature_names_most_correlated)\n",
    "    X = X.assign(id=metadata[\"id\"])\n",
    "    X = X.assign(gender=metadata[\"gender\"])\n",
    "    X = X.assign(gender=pd.get_dummies(X[\"gender\"], drop_first=True)[\"Male\"])\n",
    "    X = X.assign(age=metadata[\"age\"])\n",
    "    X = X.assign(adhd=target)\n",
    "    cols = list(X.columns)\n",
    "    col_order = [cols[-4]] + [cols[-1]] + [cols[-2]] + [cols[-3]] + cols[:-4]\n",
    "    X = X[col_order]\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 features remaining\n"
     ]
    }
   ],
   "source": [
    "X = most_correlated_features(features, metadata_sorted, feature_names, p_val=.001)\n",
    "print(X.shape[1]-2, \"features remaining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and hold out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'adhd', 'age', 'gender',\n",
       "       'Right_Occipital_Pole_2_to_Left_Cerebellum_Crus_II_0',\n",
       "       'Right_Occipital_Fusiform_Gyrus_0_to_Right_Cerebellum_VIIb_1',\n",
       "       'Right_Frontal_Pole_11_to_Left_Cerebellum_VIIb_0',\n",
       "       'Right_Lateral_Occipital_Cortex_inferior_division_2_to_Cerebellum_Vermis_VI_0',\n",
       "       'Left_Lateral_Occipital_Cortex_inferior_division_2_to_Right_Lateral_Occipital_Cortex_inferior_division_1',\n",
       "       'Left_Frontal_Pole_8_to_Left_Lingual_Gyrus_0',\n",
       "       ...\n",
       "       'Left_Middle_Temporal_Gyrus_posterior_division_0_to_Right_Pallidum_0',\n",
       "       'Right_Occipital_Fusiform_Gyrus_0_to_Left_Cerebellum_Crus_II_0',\n",
       "       'Left_Lateral_Occipital_Cortex_inferior_division_2_to_Right_Middle_Temporal_Gyrus_posterior_division_1',\n",
       "       'Hypothalamus_0_to_Right_Pallidum_0',\n",
       "       'Left_Middle_Temporal_Gyrus_posterior_division_0_to_Left_Pallidum_0',\n",
       "       'Right_Occipital_Fusiform_Gyrus_0_to_Brain-Stem_1',\n",
       "       'Left_Lateral_Occipital_Cortex_inferior_division_1_to_Right_Cerebellum_VIIb_1',\n",
       "       'Right_Cerebellum_VI_0_to_Left_Cerebellum_VIIIa_0',\n",
       "       'Left_Cingulate_Gyrus_posterior_division_1_to_Left_Occipital_Pole_0',\n",
       "       'Left_Precentral_Gyrus_3_to_Right_Lateral_Occipital_Cortex_superior_division_0'],\n",
       "      dtype='object', length=109)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.drop(columns=[\"adhd\"]), X[\"adhd\"], test_size=.2, random_state=2)\n",
    "\n",
    "X_train_ids = X_train[\"id\"].values\n",
    "X_train = X_train.drop(columns=[\"id\"])\n",
    "\n",
    "X_test_ids = X_test[\"id\"].values\n",
    "X_test = X_test.drop(columns=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard scale the data\n",
    "scale = True\n",
    "if scale:\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train positive class ratio: 0.368\n",
      "test positive class ratio: 0.356\n"
     ]
    }
   ],
   "source": [
    "# percentage of subjects with adhd in the training and testing sets\n",
    "print(\"train positive class ratio: {:.3f}\".format(np.count_nonzero(y_train == 1)/len(y_train)))\n",
    "print(\"test positive class ratio: {:.3f}\".format(np.count_nonzero(y_test == 1)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to print nice confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from the class-imbalance notebook in the Metis curriculum\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=18, percent=True):\n",
    "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    confusion_matrix: numpy.ndarray\n",
    "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "        Similarly constructed ndarrays can also be used.\n",
    "    class_names: list\n",
    "        An ordered list of class names, in the order they index the given confusion matrix.\n",
    "    figsize: tuple\n",
    "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "        the second determining the vertical size. Defaults to (10,7).\n",
    "    fontsize: int\n",
    "        Font size for axes labels. Defaults to 14.\n",
    "    percent: bool\n",
    "        whether to output percentages for each true class rather than raw values\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The resulting confusion matrix figure\n",
    "    \"\"\"\n",
    "    if percent:\n",
    "        percent_matrix = np.empty((2,2), dtype=float)\n",
    "        for row in range(2):\n",
    "            percent_matrix[row, :] = confusion_matrix[row, :]/np.sum(confusion_matrix[row, :])\n",
    "        confusion_matrix = percent_matrix\n",
    "    \n",
    "    df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        if percent:\n",
    "            heatmap = sns.heatmap(df_cm, annot=True, fmt=\".1%\")\n",
    "        else:\n",
    "            heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains the specified models and prints a confusion matrix for test set predictions\n",
    "def test_models(models, X_train, y_train, X_test, y_test):\n",
    "    for model_name in list(models.keys()):\n",
    "        m = models[model_name]\n",
    "        m.fit(X_train, y_train)\n",
    "        \n",
    "        preds = m.predict(X_test)\n",
    "        \n",
    "        roc_auc = roc_auc_score(y_test, preds)\n",
    "        \n",
    "        print(\"{}:\".format(model_name))\n",
    "        print(\"train acc = {:.3f}\".format(m.score(X_train, y_train)))\n",
    "        print(\"test acc = {:.3f}\".format(m.score(X_test, y_test)))\n",
    "        print(confusion_matrix(y_test, preds))\n",
    "        print(\"ROC AUC = {:.3f}\".format(roc_auc))\n",
    "        print(\"-----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression:\n",
      "train acc = 0.800\n",
      "test acc = 0.663\n",
      "[[48 19]\n",
      " [16 21]]\n",
      "ROC AUC = 0.642\n",
      "-----------------------\n",
      "KNN:\n",
      "train acc = 0.726\n",
      "test acc = 0.615\n",
      "[[55 12]\n",
      " [28  9]]\n",
      "ROC AUC = 0.532\n",
      "-----------------------\n",
      "SVM:\n",
      "train acc = 0.887\n",
      "test acc = 0.644\n",
      "[[66  1]\n",
      " [36  1]]\n",
      "ROC AUC = 0.506\n",
      "-----------------------\n",
      "Naive Bayes:\n",
      "train acc = 0.752\n",
      "test acc = 0.596\n",
      "[[44 23]\n",
      " [19 18]]\n",
      "ROC AUC = 0.572\n",
      "-----------------------\n",
      "Random forest:\n",
      "train acc = 0.745\n",
      "test acc = 0.644\n",
      "[[67  0]\n",
      " [37  0]]\n",
      "ROC AUC = 0.500\n",
      "-----------------------\n",
      "Gradient boosting machine:\n",
      "train acc = 1.000\n",
      "test acc = 0.635\n",
      "[[52 15]\n",
      " [23 14]]\n",
      "ROC AUC = 0.577\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "models = {\"Logistic regression\": LogisticRegression(),\n",
    "          \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "          \"SVM\": svm.SVC(kernel=\"rbf\"),\n",
    "          \"Naive Bayes\": GaussianNB(), \n",
    "          \"Random forest\": RandomForestClassifier(max_depth=4), \n",
    "          \"Gradient boosting machine\": LGBMClassifier(max_depth=4)\n",
    "         }\n",
    "test_models(models, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversample the minority class\n",
    "All of these classifiers have horrible positive class precision due to small positive class. If seperating subjects by gender, males already have an almost even split, this won't do anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled, y_train_resampled = RandomOverSampler(random_state=0).fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression:\n",
      "train acc = 0.802\n",
      "test acc = 0.625\n",
      "[[42 25]\n",
      " [14 23]]\n",
      "ROC AUC = 0.624\n",
      "-----------------------\n",
      "KNN:\n",
      "train acc = 0.745\n",
      "test acc = 0.606\n",
      "[[47 20]\n",
      " [21 16]]\n",
      "ROC AUC = 0.567\n",
      "-----------------------\n",
      "SVM:\n",
      "train acc = 0.975\n",
      "test acc = 0.673\n",
      "[[53 14]\n",
      " [20 17]]\n",
      "ROC AUC = 0.625\n",
      "-----------------------\n",
      "Naive Bayes:\n",
      "train acc = 0.724\n",
      "test acc = 0.615\n",
      "[[42 25]\n",
      " [15 22]]\n",
      "ROC AUC = 0.611\n",
      "-----------------------\n",
      "Random forest:\n",
      "train acc = 1.000\n",
      "test acc = 0.654\n",
      "[[59  8]\n",
      " [28  9]]\n",
      "ROC AUC = 0.562\n",
      "-----------------------\n",
      "Gradient boosting machine:\n",
      "train acc = 1.000\n",
      "test acc = 0.644\n",
      "[[53 14]\n",
      " [23 14]]\n",
      "ROC AUC = 0.585\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "models = {\"Logistic regression\": LogisticRegression(),\n",
    "          \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "          \"SVM\": svm.SVC(kernel=\"rbf\"),\n",
    "          \"Naive Bayes\": GaussianNB(), \n",
    "          \"Random forest\": RandomForestClassifier(), \n",
    "          \"Gradient boosting machine\": LGBMClassifier(max_depth=4)\n",
    "         }\n",
    "test_models(models, X_train_resampled, y_train_resampled, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic oversampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_adasyn, y_adasyn = ADASYN(random_state=0).fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"Logistic regression\": LogisticRegression(),\n",
    "          \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "          \"SVM\": svm.SVC(kernel=\"rbf\"),\n",
    "          \"Naive Bayes\": GaussianNB(), \n",
    "          \"Random forest\": RandomForestClassifier(), \n",
    "          \"Gradient boosting machine\": LGBMClassifier(max_depth=4)\n",
    "         }\n",
    "test_models(models, X_adasyn, y_adasyn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416, 24)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc = 0.995\n",
      "test acc = 0.644\n",
      "ROC AUC = 0.614\n",
      "[[48 19]\n",
      " [18 19]]\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(5,), max_iter=2000)\n",
    "mlp.fit(X_train, y_train)\n",
    "print(\"train acc = {:.3f}\".format(mlp.score(X_train, y_train)))\n",
    "print(\"test acc = {:.3f}\".format(mlp.score(X_test, y_test)))\n",
    "print(\"ROC AUC = {:.3f}\".format(roc_auc_score(mlp.predict(X_test), y_test)))\n",
    "print(confusion_matrix(y_test, mlp.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
