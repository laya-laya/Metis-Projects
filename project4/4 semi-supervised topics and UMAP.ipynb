{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-supervised topic modeling\n",
    "In this notebook I will try semi-supervised topic modeling. Topics are built around seed words provided for each topic. Implemented in Corex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fisher/miniconda3/envs/metis/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Users/fisher/miniconda3/envs/metis/lib/python3.7/site-packages/umap/spectral.py:4: NumbaDeprecationWarning: \u001b[1mNo direct replacement for 'numba.targets' available. Visit https://gitter.im/numba/numba-dev to request help. Thanks!\u001b[0m\n",
      "  import numba.targets\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "import scipy.sparse\n",
    "\n",
    "from scipy.interpolate import splrep, splev\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from corextopic import corextopic as ct\n",
    "from corextopic import vis_topic as vt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the speeches\n",
    "# large file, takes a while to load\n",
    "load_spacy_speeches_df = True\n",
    "if load_spacy_speeches_df:\n",
    "    with open(\"spacy_speeches_df.pkl\", \"rb\") as f:\n",
    "        speeches_df = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataframe of speeches that were given after 1900\n",
    "speeches_post_1900 = speeches_df[speeches_df[\"date\"] >= datetime.strptime(\"1900, 01, 01\", \"%Y, %m, %d\")]\n",
    "speeches_post_1900.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = text.ENGLISH_STOP_WORDS.union([\"pron\", \"president\", \"year\", \"happen\", \"thing\", \"let\", \"shall\", \"say\",\n",
    "                                           \"henceforth\", \"heretofore\", \"probably\", \"come\", \"ought\", \"shown\",\n",
    "                                           \"whereof\"])\n",
    "\n",
    "# count vectorizer on the lemmatized text with no named entities\n",
    "cv = CountVectorizer(stop_words=stop_words, min_df=3, max_df=0.8, \n",
    "                     ngram_range=(1,3), token_pattern=\"\\\\b[a-z][a-z][a-z]+\\\\b\") # only include 3+ letter words\n",
    "data_cv = cv.fit_transform(speeches_post_1900[\"lemmatized_no_ents\"])\n",
    "dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "tdm = dtm.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stuff for the corex function\n",
    "words = list(np.asarray(cv.get_feature_names()))\n",
    "\n",
    "# create a list of anchor words to seed the topic formations\n",
    "anchor_words = [[\"immigration\", \"border\"], [\"health\", \"care\"], [\"national\", \"security\"], \n",
    "               [\"economy\"], [\"trade\"], [\"education\"], [\"war\", \"military\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<corextopic.corextopic.Corex at 0x1a1e64b8d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_hidden is the number of topics\n",
    "topic_model = ct.Corex(n_hidden=7, words=words, seed=42)\n",
    "topic_model.fit(data_cv, words=words, docs=speeches_post_1900.content.values, anchors=anchor_words, anchor_strength=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: immigration, border, email, bring job, obamacare, folk, repeal replace, deplete, everybody, isis\n",
      "1: care, health, tax, taxis, money, regulation, business, cut, crime, percent\n",
      "2: security, national, reduce, program, meet, proposal, problem, social, plan, provide\n",
      "3: economy, budget, growth, economic, investment, market, cost, revenue, inflation, development\n",
      "4: trade, manufacture, wall, protect, sell, away, world trade, partnership, open, small\n",
      "5: education, low, deficit, rate, product, change, fund, worker, create, strong\n",
      "6: military, war, policy, middle, north, use, win, turn, defense, political\n"
     ]
    }
   ],
   "source": [
    "# Print all topics from the CorEx topic model\n",
    "topics = topic_model.get_topics(n_words=10)\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ', '.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(topic_model.tcs.shape[0]), topic_model.tcs, color='#4e79a7', width=0.5)\n",
    "plt.xlabel('Topic', fontsize=16)\n",
    "plt.ylabel('Total Correlation (nats)', fontsize=16)\n",
    "plt.title(\"topic correlation (higher is better)\", fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the topics back into the dataframe\n",
    "topic_bool = topic_model.labels.T.astype(int)\n",
    "topic_cols = {\"topic {}\".format(idx+1): bools for idx, bools in enumerate(topic_bool)}\n",
    "\n",
    "speeches_with_topics = pd.concat([speeches_post_1900, pd.DataFrame(topic_cols)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(speeches_with_topics.columns)\n",
    "cols[4:11] = [\"immigration\", \"health care\", \"national security\", \"economy\", \"trade\", \"education\",  \"conflict\"]\n",
    "speeches_with_topics.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get presidents in the right order\n",
    "presidents = speeches_with_topics[\"speaker\"].unique()\n",
    "presidents = presidents[[0,1,2,3,4,5,6,7,8,10,11,12,9,14,15,13,16,17,18,19,20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate percentage of speeches including a topic for each president\n",
    "percents_by_president_by_topic = {}\n",
    "for idx, topic_name in enumerate(speeches_with_topics.columns[4:]):\n",
    "    percents_by_president = {}\n",
    "    for pres in presidents:\n",
    "        speeches_this_topic_this_pres = speeches_with_topics[topic_name][speeches_with_topics[\"speaker\"]==pres].values\n",
    "        in_topic = sum(speeches_this_topic_this_pres)\n",
    "        percent = in_topic/len(speeches_this_topic_this_pres)\n",
    "        percents_by_president[pres] = percent\n",
    "    percents_by_president_by_topic[topic_name] = percents_by_president"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,16))\n",
    "for idx, topic in enumerate(list(percents_by_president_by_topic.keys())):\n",
    "    plt.subplot(3,3,idx+1)\n",
    "    presidents = list(percents_by_president_by_topic[topic].keys())\n",
    "    percents = list(percents_by_president_by_topic[topic].values())\n",
    "    plt.bar(presidents, percents)\n",
    "    plt.ylabel(\"percent of speeches incl. topic\")\n",
    "    plt.xticks(rotation=40)\n",
    "    plt.title(topic, fontsize=18)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"topics_by_pres.png\", dpi=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"economy\"\n",
    "plt.figure(figsize=(12,8))\n",
    "presidents = list(percents_by_president_by_topic[topic].keys())\n",
    "percents = list(percents_by_president_by_topic[topic].values())\n",
    "plt.bar(presidents, percents)\n",
    "plt.ylabel(\"percent\\nof\\nspeeches\\nincluding\\ntopic\", fontsize=16, rotation=0, labelpad=45)\n",
    "plt.xticks(rotation=50, fontsize=16)\n",
    "plt.title(\"'Economy' Topic by President\", fontsize=26)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"economy_topic.png\", dpi=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fisher/miniconda3/envs/metis/lib/python3.7/site-packages/numba/np/ufunc/parallel.py:355: NumbaWarning: \u001b[1mThe TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 11000. The TBB threading layer is disabled.\u001b[0m\n",
      "  warnings.warn(problem)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-07940dab2b14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mreducer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUMAP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreducer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeeches_with_topics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/metis/lib/python3.7/site-packages/umap/umap_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1935\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_metric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"euclidean\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"l2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1936\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1937\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1938\u001b[0m         )[inverse]\n\u001b[1;32m   1939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/metis/lib/python3.7/site-packages/umap/umap_.py\u001b[0m in \u001b[0;36msimplicial_set_embedding\u001b[0;34m(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, output_metric, output_metric_kwds, euclidean_output, parallel, verbose)\u001b[0m\n\u001b[1;32m   1089\u001b[0m             \u001b[0mnegative_sample_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m             \u001b[0mparallel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m         )\n\u001b[1;32m   1093\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/metis/lib/python3.7/site-packages/umap/layouts.py\u001b[0m in \u001b[0;36moptimize_layout_euclidean\u001b[0;34m(head_embedding, tail_embedding, head, tail, n_epochs, n_vertices, epochs_per_sample, a, b, rng_state, gamma, initial_alpha, negative_sample_rate, parallel, verbose)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mepoch_of_next_negative_sample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mepoch_of_next_sample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         )\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reducer = umap.UMAP(random_state=42)\n",
    "reducer.fit(speeches_with_topics.iloc[:,4:].values);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = reducer.transform(speeches_with_topics.iloc[:,4:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = speeches_with_topics.columns[4:]\n",
    "\n",
    "# add a column showing if a speech was not assigned a topic for visualization purposes\n",
    "speeches_with_topics[\"no topic\"] = (speeches_with_topics[topic_names].values.sum(axis=1) == 0).astype(int)\n",
    "len(speeches_with_topics[speeches_with_topics[\"no topic\"]==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = speeches_with_topics.columns[4:].values\n",
    "\n",
    "plt.figure(figsize=(26,14))\n",
    "for idx, topic in enumerate(topic_names):\n",
    "    plt.subplot(2,4,idx+1)\n",
    "    colors = [sns.color_palette()[x] for x in speeches_with_topics[topic]]\n",
    "    plt.scatter(embedding[:, 0], embedding[:, 1], alpha=.5, c=colors)\n",
    "    plt.title(topic, fontsize=16);\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"umaps.png\", dpi=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10))\n",
    "colors = [sns.color_palette()[x] for x in (speeches_with_topics[\"speaker\"]==\"trump\").astype(int)]\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], alpha=.5, c=colors);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
