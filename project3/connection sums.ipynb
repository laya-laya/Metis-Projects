{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum connection strengths\n",
    "Following the steps of [this](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5526681/) paper, I tried generating just two features from all of the connection strengths:\n",
    "* sum of positive connections\n",
    "* sum of negative connections\n",
    "\n",
    "### Outcome:\n",
    "This method was unsuccessful, ROC AUC and accuracy scores decreased significantly. This method is not used in my final modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict\n",
    "import os\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conn_matrices():\n",
    "    file_names = os.listdir(\"data/ADHD200_CC200\")\n",
    "\n",
    "    cm_file_re = r\"^\\S+connectivity_matrix_file\\.txt$\"\n",
    "\n",
    "    conn_matrices = OrderedDict()\n",
    "    for file_name in file_names:\n",
    "        if re.match(cm_file_re, file_name):\n",
    "            id_ = \"\".join(file_name.split(\"_\")[:-3])\n",
    "        \n",
    "            cm = np.empty((190,190))\n",
    "            with open(\"data/ADHD200_CC200/{}\".format(file_name)) as f:\n",
    "                for idx, row in enumerate(f):\n",
    "                    row = row.strip().split(\" \")\n",
    "                    row = list(map(np.float, row))\n",
    "                    cm[idx, :] = row\n",
    "        \n",
    "            conn_matrices[id_] = cm\n",
    "    return conn_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regions():\n",
    "    \"\"\"\n",
    "    Gets the names of the regions (in order of appearance in connectivity matrix). \n",
    "    All files have the same order of regions, so we only to need to get this once.\n",
    "    Some region names are repeated because there are multiple points within that region,\n",
    "        so numbers are appended to the region names to distinguish them.\n",
    "    \n",
    "    returns a list of strings\n",
    "    \"\"\"\n",
    "    regions_path = \"data/ADHD200_CC200/KKI_1018959_region_names_full_file.txt\"\n",
    "    regions = []\n",
    "    with open(regions_path, \"r\") as f:\n",
    "        regions = [region.strip().replace(\" \", \"_\") for region in f]\n",
    "    names = defaultdict(int)\n",
    "    distinct_region_names = []\n",
    "    for region in regions:\n",
    "        distinct_region_names.append(region+\"_\"+str(names[region]))\n",
    "        names[region] += 1\n",
    "    return distinct_region_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_names = get_regions()\n",
    "conn_matrices = get_conn_matrices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get subject data\n",
    "with open(\"cm_table.html\", \"r\") as f:\n",
    "    table = f.read()\n",
    "\n",
    "soup = BeautifulSoup(table, \"html.parser\")\n",
    "\n",
    "rows = soup.find_all(class_=\"powerTable\")[1].tbody.find_all(\"tr\")[3:523]\n",
    "\n",
    "cols = defaultdict(list)\n",
    "for row in rows:\n",
    "    text_list = list(row.stripped_strings)\n",
    "    if len(text_list) == 13:\n",
    "        text_list.insert(7, 'na') # insert so list is standard size when that column was empty on the webpage\n",
    "    cols[\"study\"].append(text_list[2])\n",
    "    cols[\"id\"].append(text_list[3].replace(\"_\", \"\"))\n",
    "    cols[\"age\"].append(float(text_list[8]))\n",
    "    cols[\"gender\"].append(text_list[10])\n",
    "    cols[\"label\"].append(text_list[11])\n",
    "\n",
    "subject_data = pd.DataFrame(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_subject_data(subject_data, subject_order):\n",
    "    \"\"\"\n",
    "    Sorts a dataframe by the order given\n",
    "    :arg subject_data: dataframe with ADHD200 subject data\n",
    "    :arg subject_order: a list of subjects in specific order\n",
    "    \"\"\"\n",
    "    subject_data_ids = subject_data[\"id\"].values\n",
    "    subjects_order_in_subject_data = [np.where(subject_data_ids==subject)[0][0] for subject in subject_order]\n",
    "    subject_data_sort = subject_data.iloc[subjects_order_in_subject_data, :]\n",
    "    return subject_data_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_order = list(conn_matrices.keys())\n",
    "subject_data_sorted = sort_subject_data(subject_data, subject_order)\n",
    "adhd = [0 if label == \"Typically Developing\" else 1 for label in subject_data_sorted[\"label\"]]\n",
    "subject_data_sorted = subject_data_sorted.assign(adhd=adhd).drop(columns=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_conn_matrices(conn_matrices_dict, region_names):\n",
    "    \"\"\"\n",
    "    Flatten a cm dictionary (mapping subjects to connectivity matrices), such that each unique value in the\n",
    "        connectivity matrix is a column feature in a row.\n",
    "    Returns: 1) a numpy array where each row represents a subject with each column a feature;\n",
    "             2) a list of the subject ids in the order they appear in the feature array;\n",
    "             3) a list of the feature names in the order they appear in the feature array.\n",
    "    The subjects list holds the row labels, feature_names list holds column labels.\n",
    "    \"\"\"\n",
    "    subjects = list(conn_matrices_dict.keys())\n",
    "    num_rows = len(subjects)\n",
    "    features = np.empty((num_rows, 17955))\n",
    "    \n",
    "    # adjacency matrices have duplicate values, only need values from half of the matrix (and don't need diagonal)\n",
    "    # np.tril_indices() returns indices of unique values\n",
    "    row_idxs, col_idxs = np.tril_indices(190, k=-1)\n",
    "    for idx, subject in enumerate(subjects):\n",
    "        cm = conn_matrices_dict[subject]\n",
    "        row = np.array([cm[row_idx, col_idx] for row_idx, col_idx in zip(row_idxs, col_idxs)])\n",
    "        features[idx, :] = row\n",
    "    \n",
    "    feature_names = [region_names[row_idx]+\"_to_\"+region_names[col_idx] \n",
    "                     for row_idx, col_idx in zip(row_idxs, col_idxs)]\n",
    "    \n",
    "    return features, subjects, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, subjects, feature_names = flatten_conn_matrices(conn_matrices, region_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_correlated_features(features, metadata, feature_names, p_val=.01):\n",
    "    \"\"\"\n",
    "    returns a DataFrame with a subset of the features which have a correlation p value less than the specified cutoff\n",
    "    :arg features: numpy feature matrix, sorted in the same order as the metadata.\n",
    "    :arg target: DataFrame with target and ids, sorted in the same order as the feature matrix.\n",
    "    :arg feature_names: the names of the features in the feature matrix, same order.\n",
    "    :arg p_val: the maximum p value for a feature to be included.\n",
    "    \"\"\"\n",
    "    # get the p values for correlations. lower is better!\n",
    "    target=metadata[\"adhd\"] \n",
    "    correlation_p_vals = np.array([pearsonr(features[:,col], target)[1] for col in range(features.shape[1])])\n",
    "    # get the order of columns which are most correlated with having adhd\n",
    "    corr_p_vals_argsort = correlation_p_vals.argsort()\n",
    "    # the number of features with correlation p values less than the cutoff\n",
    "    num_features = np.count_nonzero(correlation_p_vals < p_val)\n",
    "    # get the indices of features of features with p vals less than the cutoff\n",
    "    most_correlated = corr_p_vals_argsort[:num_features]\n",
    "    \n",
    "    features_most_correlated = features[:, most_correlated]\n",
    "    feature_names_most_correlated = [feature_names[idx] for idx in most_correlated]\n",
    "    \n",
    "    # make features dataframe with the smaller features\n",
    "    X = pd.DataFrame(features_most_correlated, columns=feature_names_most_correlated)\n",
    "    X = X.assign(adhd=target.values)\n",
    "    X = X.assign(id=metadata[\"id\"].values)\n",
    "    X = X.assign(gender=metadata[\"gender\"].values)\n",
    "    X = X.assign(gender=pd.get_dummies(X[\"gender\"], drop_first=True)[\"Male\"])\n",
    "    X = X.assign(age=metadata[\"age\"].values)\n",
    "    cols = list(X.columns)\n",
    "    col_order = [cols[-3]] + [cols[-4]] + [cols[-1]] + [cols[-2]] + cols[:-4]\n",
    "    X = X[col_order]\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = most_correlated_features(features, subject_data_sorted, feature_names, p_val=.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum connection values:\n",
    "Two features will be created from all of the significant connections: a sum of the positive values, and a sum of the negative values. This method is recommended in https://www.nature.com/articles/nprot.2016.178"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Right_Lateral_Occipital_Cortex_inferior_division_2_to_Cerebellum_Vermis_VI_0        -0.228602\n",
       "Right_Occipital_Pole_2_to_Left_Cerebellum_Crus_II_0                                  0.185352\n",
       "Left_Cingulate_Gyrus_posterior_division_0_to_Right_Frontal_Pole_0                    0.202713\n",
       "Left_Lateral_Occipital_Cortex_superior_division_2_to_Right_Frontal_Pole_0           -0.128154\n",
       "Left_Frontal_Pole_0_to_Right_Frontal_Pole_0                                         0.0335659\n",
       "                                                                                      ...    \n",
       "Left_Thalamus_0_to_Left_Cerebellum_VIIb_0                                            0.173973\n",
       "Right_Frontal_Pole_2_to_Left_Frontal_Orbital_Cortex_0                               -0.216298\n",
       "Left_Middle_Temporal_Gyrus_posterior_division_1_to_Left_Frontal_Orbital_Cortex_0     0.172591\n",
       "Left_Lateral_Occipital_Cortex_superior_division_2_to_Left_Cerebellum_Crus_II_1       0.323969\n",
       "Left_Frontal_Pole_7_to_Right_Supramarginal_Gyrus_posterior_division_0               -0.158088\n",
       "Name: 0, Length: 1835, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0,:][4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.assign(pos_sum=data.apply(lambda row: np.sum([val for val in row.values[4:] if val > 0]), axis=1))\n",
    "data = data.assign(neg_sum=data.apply(lambda row: np.sum([val for val in row.values[4:] if val < 0]), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[[\"pos_sum\", \"neg_sum\"]].values\n",
    "y = data[\"adhd\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc = 0.627\n",
      "test acc = 0.615\n",
      "ROC AUC = 0.496\n",
      "[[62  4]\n",
      " [36  2]]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "roc_auc = roc_auc_score(y_test, preds)\n",
    "\n",
    "print(\"train acc = {:.3f}\".format(model.score(X_train, y_train)))\n",
    "print(\"test acc = {:.3f}\".format(model.score(X_test, y_test)))\n",
    "print(\"ROC AUC = {:.3f}\".format(roc_auc))\n",
    "print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc = 0.630\n",
      "test acc = 0.606\n",
      "ROC AUC = 0.500\n",
      "[[59  7]\n",
      " [34  4]]\n"
     ]
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "roc_auc = roc_auc_score(y_test, preds)\n",
    "\n",
    "print(\"train acc = {:.3f}\".format(model.score(X_train, y_train)))\n",
    "print(\"test acc = {:.3f}\".format(model.score(X_test, y_test)))\n",
    "print(\"ROC AUC = {:.3f}\".format(roc_auc))\n",
    "print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting just based on sum of connection values doesn't work well at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
