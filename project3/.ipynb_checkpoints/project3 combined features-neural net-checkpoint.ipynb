{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural net\n",
    "Using the combined features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score\n",
    "from sklearn.metrics import roc_auc_score, plot_roc_curve, make_scorer, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#from lightgbm import LGBMClassifier\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, ADASYN\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load connection strength data\n",
    "Load the data then use pearsonr p values to select which features correlate most with having ADHD, and only include those features for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_connection_features.pkl\", \"rb\") as f:\n",
    "    X = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_correlated_features(X, target, p_val=.01, start_feature_idx=3):\n",
    "    \"\"\"\n",
    "    returns a DataFrame with a subset of the features which have a correlation p value less than the specified cutoff\n",
    "    \"\"\"\n",
    "    # get the p values for correlations. lower is better!\n",
    "    correlation_p_vals = np.array([pearsonr(X[col].values, target)[1] for col in \n",
    "                                                       list(X.columns[start_feature_idx:])])\n",
    "    # get the order of columns which are most correlated with having adhd\n",
    "    corr_p_vals_argsort = correlation_p_vals.argsort()\n",
    "    # the number of features with correlation p values less than the cutoff\n",
    "    num_features = np.count_nonzero(correlation_p_vals < p_val)\n",
    "    print(num_features, \"features remaining with p value {}\".format(p_val))\n",
    "    # get the indices of features of features with p vals less than the cutoff\n",
    "    most_correlated = list(corr_p_vals_argsort[:num_features])\n",
    "    \n",
    "    features_most_correlated = X.iloc[:, [0,1,2] + most_correlated]\n",
    "    \n",
    "    return features_most_correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "823 features remaining with p value 0.02\n"
     ]
    }
   ],
   "source": [
    "target = X[\"adhd\"].values\n",
    "X_correlated = most_correlated_features(X, target, p_val=.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add local network statistic features\n",
    "Load from a pickle file. These are calculated in the `project3 network analysis - local level.ipynb` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_local_node_features.pkl\", \"rb\") as f:\n",
    "    node_measures = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 features remaining with p value 0.03\n"
     ]
    }
   ],
   "source": [
    "target = X[\"adhd\"].values\n",
    "node_measures_correlated = most_correlated_features(node_measures, target, p_val=.03, start_feature_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_correlated = pd.concat([X_correlated, node_measures_correlated], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(520, 863)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_correlated.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and hold out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_correlated.drop(columns=[\"adhd\"]), \n",
    "                                                    X_correlated[\"adhd\"], test_size=.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard scale the data\n",
    "scale = True\n",
    "if scale:\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train)\n",
    "X_test = torch.from_numpy(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.zeros((y_train.size, y_train.max()+1))\n",
    "y[np.arange(y_train.size), y_train] = 1\n",
    "y_train = y\n",
    "\n",
    "y = np.zeros((y_test.size, y_test.max()+1))\n",
    "y[np.arange(y_test.size), y_test] = 1\n",
    "y_test = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([416, 862])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(862, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 2),\n",
    "            nn.Softmax())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NYU2497695           1\n",
       "KKI2917777           0\n",
       "NYU1737393           0\n",
       "KKI2344857           0\n",
       "Peking13976121       1\n",
       "                    ..\n",
       "Peking16187322       0\n",
       "Peking11879542       0\n",
       "NeuroIMAGE7504392    1\n",
       "NYU0010081           1\n",
       "Peking13233028       0\n",
       "Name: adhd, Length: 416, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.float()\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for i, (x, y) in zip(X_train, y_train):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(x)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        if (i * 128) % (128 * 100) == 0:\n",
    "            print(f'{i * 128} / 50000')\n",
    "            \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(valid_loader):\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            valid_losses.append(loss.item())\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "    mean_train_losses.append(np.mean(train_losses))\n",
    "    mean_valid_losses.append(np.mean(valid_losses))\n",
    "    \n",
    "    accuracy = 100*correct/total\n",
    "    valid_acc_list.append(accuracy)\n",
    "    print('epoch : {}, train loss : {:.4f}, valid loss : {:.4f}, valid acc : {:.2f}%'\\\n",
    "         .format(epoch+1, np.mean(train_losses), np.mean(valid_losses), accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
