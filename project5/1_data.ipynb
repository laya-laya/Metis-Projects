{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "In this notebook I'll be downloading the data and putting it into a PyTorch Geometric Data object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import gzip\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pickle as pkl\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "import pronto\n",
    "\n",
    "from Bio import Entrez\n",
    "from pyumls import api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download network data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the filenames\n",
    "urls = [\"http://snap.stanford.edu/biodata/datasets/10012/files/DG-AssocMiner_miner-disease-gene.tsv.gz\",\n",
    "        \"https://www.inetbio.org/humannet/networks/HumanNet-XN.tsv\",\n",
    "        \"http://snap.stanford.edu/biodata/datasets/10006/files/DD-Miner_miner-disease-disease.tsv.gz\",\n",
    "        \"https://raw.githubusercontent.com/DiseaseOntology/HumanDiseaseOntology/master/src/ontology/doid.obo\"\n",
    "        ]\n",
    "    \n",
    "fnames = []\n",
    "for url in urls:\n",
    "    fname = url.split(\"/\")[-1]\n",
    "    fnames.append(\"data/\"+fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the data isn't downloaded, download it into a data folder\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "    \n",
    "    for url, fname in zip(urls, fnames):\n",
    "        with open(fname, \"wb\") as f:\n",
    "            r = requests.get(url)\n",
    "            f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the disease-gene association file\n",
    "with gzip.open(fnames[0], \"rb\") as f:\n",
    "    edges = defaultdict(list)\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx == 0:\n",
    "            pass\n",
    "        else:\n",
    "            d_id, d_name, g_id = line.decode(\"utf-8\").strip().split(\"\\t\")\n",
    "            edges[\"disease_id\"].append(d_id)\n",
    "            edges[\"disease_name\"].append(d_name.strip('\"'))\n",
    "            edges[\"gene_id\"].append(int(g_id))\n",
    "    \n",
    "    disease_gene_edge_df = pd.DataFrame(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the gene-gene association file\n",
    "with open(fnames[1], \"r\") as f:\n",
    "    edges = defaultdict(list)\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx == 0:\n",
    "            pass\n",
    "        else:\n",
    "            g1_id, g2_id, weight = line.strip().split(\"\\t\")\n",
    "            g1_id = int(g1_id)\n",
    "            g2_id = int(g2_id)\n",
    "            weight = float(weight)\n",
    "            # we only want the rows where both of the genes are already in our disease-gene network\n",
    "            if (g1_id in disease_gene_edge_df[\"gene_id\"].values) and (g2_id in disease_gene_edge_df[\"gene_id\"].values):\n",
    "                edges[\"gene1_id\"].append(g1_id)\n",
    "                edges[\"gene2_id\"].append(g2_id)\n",
    "                edges[\"log_likelihood_score\"].append(weight)\n",
    "    \n",
    "    gene_gene_edge_df = pd.DataFrame(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is useful for mapping diseases labeled with different IDs\n",
    "doid = pronto.Ontology(\"data/doid.obo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the disease-disease association file\n",
    "with gzip.open(fnames[2], \"rb\") as f:\n",
    "    edges = defaultdict(list)\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx == 0:\n",
    "            headers = line.decode(\"utf-8\").strip().split(\"\\t\")\n",
    "        else:\n",
    "            doid1, doid2 = line.decode(\"utf-8\").strip().split(\"\\t\") # get the disease ids (in DOID format)\n",
    "            # get the CUI cross references (xrefs) associated with these DOIDs\n",
    "            doid1_xrefs = [xref.id.replace(\"UMLS_CUI:\", \"\") for xref in doid[doid1].xrefs if \"UMLS_CUI\" in xref.id]\n",
    "            doid2_xrefs = [xref.id.replace(\"UMLS_CUI:\", \"\") for xref in doid[doid2].xrefs if \"UMLS_CUI\" in xref.id]\n",
    "            \n",
    "            # only add the disease-disease link if both of these diseases are present in the disease-gene associations\n",
    "            for xref1 in doid1_xrefs:\n",
    "                if xref1 in disease_gene_edge_df[\"disease_id\"].values:\n",
    "                    for xref2 in doid2_xrefs:\n",
    "                        if xref2 in disease_gene_edge_df[\"disease_id\"].values:\n",
    "                            edges[\"disease1_id\"].append(xref1)\n",
    "                            edges[\"disease2_id\"].append(xref2)\n",
    "    \n",
    "    disease_disease_edge_df = pd.DataFrame(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode node ids\n",
    "Below I'm adding integer labels for each gene and each disease. These integer labels will be the node ids for the graph representation. The gene_node_ids values start where disease_node_ids leaves off so there aren't any overlap in node ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the mapping from a (disease or gene) identifier to a node id\n",
    "diseases_and_genes = np.append(disease_gene_edge_df[\"disease_id\"].values, disease_gene_edge_df[\"gene_id\"].values)\n",
    "diseases_and_genes = list(dict.fromkeys(diseases_and_genes).keys())\n",
    "node_id_mapping = {id_: node_id for id_, node_id in zip(diseases_and_genes, list(range(len(diseases_and_genes))))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add node id and gene id columns to the disease-gene association df\n",
    "disease_gene_edge_df[\"disease_node_id\"] = [node_id_mapping[id_] for id_ in disease_gene_edge_df[\"disease_id\"]]\n",
    "disease_gene_edge_df[\"gene_node_id\"] = [node_id_mapping[id_] for id_ in disease_gene_edge_df[\"gene_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add node ids to the gene-gene assocations\n",
    "gene_gene_edge_df[\"gene1_node_id\"] = [node_id_mapping[id_] for id_ in gene_gene_edge_df[\"gene1_id\"]]\n",
    "gene_gene_edge_df[\"gene2_node_id\"] = [node_id_mapping[id_] for id_ in gene_gene_edge_df[\"gene2_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add node ids to the disease-disease assocations\n",
    "disease_disease_edge_df[\"disease1_node_id\"] = [node_id_mapping[id_] for id_ in disease_disease_edge_df[\"disease1_id\"]]\n",
    "disease_disease_edge_df[\"disease2_node_id\"] = [node_id_mapping[id_] for id_ in disease_disease_edge_df[\"disease2_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases = disease_gene_edge_df.iloc[:, [0,1,3]].drop_duplicates() # df with just the diseases\n",
    "genes = disease_gene_edge_df.iloc[:, [2,4]].drop_duplicates() # df with just the genes\n",
    "\n",
    "diseases.to_pickle(\"data/diseases_df.pkl\")\n",
    "genes.to_pickle(\"data/genes_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get gene and disease features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gene features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Entrez module of Biopython will be used for accessing gene data\n",
    "# enter your email below when requesting data from the API\n",
    "# Entrez.email ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed from the BioPython docs: https://biopython.org/wiki/Annotate_Entrez_Gene_IDs.\n",
    "def retrieve_annotation(id_list):\n",
    "    \"\"\"\n",
    "    Annotates Entrez Gene IDs using Bio.Entrez, in particular epost (to\n",
    "    submit the data to NCBI) and esummary to retrieve the information.\n",
    "    Returns a list of dictionaries with the annotations.\n",
    "    \"\"\"\n",
    "    \n",
    "    id_list = list(map(str, id_list))\n",
    "    \n",
    "    request = Entrez.epost(\"gene\", id=\",\".join(id_list))\n",
    "    try:\n",
    "        result = Entrez.read(request)\n",
    "    except RuntimeError as e:\n",
    "        print(\"An error occurred while retrieving the annotations.\")\n",
    "        print(\"The error returned was %s\" % e)\n",
    "        return\n",
    "\n",
    "    webEnv = result[\"WebEnv\"]\n",
    "    queryKey = result[\"QueryKey\"]\n",
    "    data = Entrez.esummary(db=\"gene\", webenv=webEnv, query_key=queryKey)\n",
    "    annotations = Entrez.read(data)\n",
    "\n",
    "    print(\"Retrieved %d annotations for %d genes\" % (len(annotations), len(id_list)))\n",
    "\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = genes[\"gene_id\"].to_list()\n",
    "\n",
    "if not os.path.isfile(\"gene_summaries.pkl\"):\n",
    "    annotations = retrieve_annotation(id_list)\n",
    "    annotation_list = annotations[\"DocumentSummarySetmentSummarySet\"][\"DocumentSummary\"]\n",
    "    gene_summaries = [str(doc[\"Summary\"]) for doc in annotation_list]\n",
    "    with open(\"gene_summaries.pkl\", \"wb\") as f:\n",
    "        pkl.dump(gene_summaries, f)\n",
    "else:\n",
    "    with open(\"gene_summaries.pkl\", \"rb\") as f:\n",
    "        gene_summaries = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', '', text) # remove text within < >\n",
    "    text = re.sub(r'\\[.*?\\]', '', text) # remove text within [ ]\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text) # remove punctuation\n",
    "    text = re.sub(r'[‘’“”…]', '', text)\n",
    "    text = re.sub(r'\\s\\d+\\s', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "genes[\"summary\"] = gene_summaries\n",
    "genes[\"summary\"] = genes[\"summary\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fisher/miniconda3/envs/uno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:382: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['counter', 'effect', 'effects'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "# stopwords from https://cs.stanford.edu/people/sonal/gupta14jamia_supl.pdf, plus some custom words\n",
    "medical_stopwords = \"gene, genes, protein, refseq, provided, contains, encoded, encode, encoding, splicing, type, expression, located, superfamily, target, known, described, identified, including, thought, syndrome, family, associated, region, domain, alternative, alternatively, factor, transcription, cause, belongs, belong, activity, encodes, variants, transcript, cell, proteins, multiple, member, involved, different, role, results, cells, function,  disease, diseases, disorder, symptom, symptoms, drug, drugs, problems, problem,prob, probs, med, meds,pill,  pills,  medicine,  medicines,  medication,  medications,  treatment,  treatments,  caps,  capsules,  capsule, tablet,  tablets,  tabs,  doctor,  dr,  dr.,  doc,  physician,  physicians,  test,  tests,  testing,  specialist, specialists, side-effect, side-effects, pharmaceutical, pharmaceuticals, pharma, diagnosis, diagnose, diagnosed, exam, challenge,  device,  condition,  conditions,  suffer,  suffering  ,suffered,  feel,  feeling,  prescription,  prescribe, prescribed, over-the-counter, otc\"\n",
    "medical_stopwords = list(map(str.strip, medical_stopwords.split(\",\")))\n",
    "\n",
    "stop_words = ENGLISH_STOP_WORDS.union(medical_stopwords)\n",
    "\n",
    "# count vectorizer on the lemmatized text with no named entities\n",
    "cv = TfidfVectorizer(stop_words=stop_words, min_df=2, max_df=0.8, ngram_range=(1,1))\n",
    "data_cv = cv.fit_transform(genes[\"summary\"])\n",
    "dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = pd.concat([genes.reset_index(drop=True), dtm], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disease features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This list was curated using the UMLS UTS api, which I won't show here. (https://uts.nlm.nih.gov/home.html)\n",
    "with open(\"disease_definitions.pkl\", \"rb\") as f:\n",
    "    disease_definitions = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases[\"definition\"] = disease_definitions\n",
    "diseases[\"definition\"] = diseases[\"definition\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fisher/miniconda3/envs/uno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:382: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['counter', 'effect', 'effects'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "# stopwords from https://cs.stanford.edu/people/sonal/gupta14jamia_supl.pdf, plus some custom words\n",
    "medical_stopwords = \"gene, genes, protein, refseq, provided, characterized, contains, encoded, encode, encoding, splicing, type, expression, located, superfamily, target, known, described, identified, including, thought, syndrome, family, associated, region, domain, alternative, alternatively, factor, transcription, cause, belongs, belong, activity, encodes, variants, transcript, cell, proteins, multiple, member, involved, different, role, results, cells, function,  disease, diseases, disorder, symptom, symptoms, drug, drugs, problems, problem,prob, probs, med, meds,pill,  pills,  medicine,  medicines,  medication,  medications,  treatment,  treatments,  caps,  capsules,  capsule, tablet,  tablets,  tabs,  doctor,  dr,  dr.,  doc,  physician,  physicians,  test,  tests,  testing,  specialist, specialists, side-effect, side-effects, pharmaceutical, pharmaceuticals, pharma, diagnosis, diagnose, diagnosed, exam, challenge,  device,  condition,  conditions,  suffer,  suffering  ,suffered,  feel,  feeling,  prescription,  prescribe, prescribed, over-the-counter, otc\"\n",
    "medical_stopwords = list(map(str.strip, medical_stopwords.split(\",\")))\n",
    "\n",
    "stop_words = ENGLISH_STOP_WORDS.union(medical_stopwords)\n",
    "\n",
    "# count vectorizer on the lemmatized text with no named entities\n",
    "cv = TfidfVectorizer(stop_words=stop_words, min_df=2, max_df=0.8, ngram_range=(1,1))\n",
    "data_cv = cv.fit_transform(diseases[\"definition\"])\n",
    "dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases = pd.concat([diseases.reset_index(drop=True), dtm], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to NetworkX graphs\n",
    "This is a much more natural way to represent this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below I create the building blocks for the graphs: the different edge sets (disease -> disease, disease -> gene, and gene -> gene), as well as node sets (disease, gene, and disease+gene)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_gene_edges = list(zip(disease_gene_edge_df[\"disease_node_id\"].values, \n",
    "                              disease_gene_edge_df[\"gene_node_id\"].values))\n",
    "\n",
    "gene_gene_edges = list(zip(gene_gene_edge_df[\"gene1_node_id\"].values, \n",
    "                              gene_gene_edge_df[\"gene2_node_id\"].values))\n",
    "\n",
    "disease_disease_edges = list(zip(disease_disease_edge_df[\"disease1_node_id\"].values, \n",
    "                              disease_disease_edge_df[\"disease2_node_id\"].values))\n",
    "\n",
    "# node features are added from the dataframes\n",
    "disease_nodes = [(row[2], {\"x\": [1.,0.]+row[4:].to_list() }) for _, row in diseases.iterrows()] \n",
    "gene_nodes = [(row[1], {\"x\": [0.,1.]+row[3:].to_list() }) for _, row in genes.iterrows()] \n",
    "all_nodes = disease_nodes + gene_nodes # all nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph creation\n",
    "Here I use the pieces above to build several different graphs in NetworkX. One graph is created for each edge set, as well as a graph with all edge sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False 7813 21357\n"
     ]
    }
   ],
   "source": [
    "# disease-gene edge set graph. No node features are added bc they aren't needed in this graph\n",
    "G_disease_gene = nx.Graph()\n",
    "G_disease_gene.add_edges_from(disease_gene_edges)\n",
    "\n",
    "# make sure the graph was created as intended. Expect undirected graph with 7,813 nodes.\n",
    "print(G_disease_gene.is_directed(), G_disease_gene.number_of_nodes(), G_disease_gene.number_of_edges())\n",
    "\n",
    "nx.write_gpickle(G_disease_gene, \"data/disease_gene_graph.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False 519 82\n"
     ]
    }
   ],
   "source": [
    "# disease-disease edge set graph\n",
    "G_disease = nx.Graph()\n",
    "G_disease.add_edges_from(disease_disease_edges)\n",
    "G_disease.add_nodes_from(disease_nodes)\n",
    "\n",
    "# make sure the graph was created as intended. Expect undirected graph with 519 nodes.\n",
    "print(G_disease.is_directed(), G_disease.number_of_nodes(), G_disease.number_of_edges())\n",
    "\n",
    "nx.write_gpickle(G_disease, \"data/disease_graph.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False 7294 131509\n"
     ]
    }
   ],
   "source": [
    "# gene-gene edge set graph\n",
    "G_gene = nx.Graph()\n",
    "G_gene.add_edges_from(gene_gene_edges)\n",
    "G_gene.add_nodes_from(gene_nodes)\n",
    "\n",
    "# make sure the graph was created as intended. Expect undirected graph with 7,294 nodes.\n",
    "print(G_gene.is_directed(), G_gene.number_of_nodes(), G_gene.number_of_edges())\n",
    "\n",
    "nx.write_gpickle(G_gene, \"data/gene_graph.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False 7813 152948\n"
     ]
    }
   ],
   "source": [
    "# graph with all edge sets. This graph will only be used for visualization,\n",
    "#    so only the identity of each node (gene or disease) will be added to features.\n",
    "disease_nodes = [(row[2], {\"x\": [1.,0.]}) for _, row in diseases.iterrows()] \n",
    "gene_nodes = [(row[1], {\"x\": [0.,1.]}) for _, row in genes.iterrows()] \n",
    "all_nodes = disease_nodes + gene_nodes # all nodes\n",
    "\n",
    "G_all = nx.Graph()\n",
    "G_all.add_edges_from(gene_gene_edges)\n",
    "G_all.add_edges_from(disease_disease_edges)\n",
    "G_all.add_edges_from(disease_gene_edges)\n",
    "\n",
    "G_all.add_nodes_from(all_nodes)\n",
    "\n",
    "# make sure the graph was created as intended. Expect undirected graph with 7,294 nodes.\n",
    "print(G_all.is_directed(), G_all.number_of_nodes(), G_all.number_of_edges())\n",
    "\n",
    "nx.write_gpickle(G_all, \"data/vis_graph.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
