{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and processing speeches\n",
    "I'll be doing some text cleaning following the guidelines in (this)[https://github.com/adashofdata/nlp-in-python-tutorial/blob/master/1-Data-Cleaning.ipynb] notebook. Then spaCy will be used to tokenize the text, find lemmatizations, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "import os\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from mongo\n",
    "Just load the data if it is already saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_speeches_df = False\n",
    "if load_speeches_df:\n",
    "    with open(\"speeches_df.pkl\", \"rb\") as f:\n",
    "        speeches_df = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['speeches']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    'host': '13.56.124.215:27017',\n",
    "    'username': 'fisher',\n",
    "    'password': 'mongoPassword',\n",
    "    'authSource': 'speeches'\n",
    "}\n",
    "\n",
    "client = MongoClient(**config)\n",
    "db = client.speeches\n",
    "\n",
    "db.list_collection_names() # check the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.speeches.find_one({\"speaker\": \"trump\"})[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = db.speeches.find() # all speech documents\n",
    "speeches_dict = defaultdict(list)\n",
    "for speech in cursor:\n",
    "    speeches_dict[\"speaker\"].append(speech[\"speaker\"])\n",
    "    speeches_dict[\"date\"].append(speech[\"date\"])\n",
    "    speeches_dict[\"content\"].append(speech[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_df = pd.DataFrame(speeches_dict)\n",
    "speeches_df.sort_values(by=\"date\", ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_speeches_df = False\n",
    "if save_speeches_df:\n",
    "    with open(\"speeches_df.pkl\", \"wb\") as f:\n",
    "        pkl.dump(speeches_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and processing with spaCy\n",
    "Just load the dataframe if it has already been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_spacy_speeches_df = False\n",
    "# large file, takes a while to load\n",
    "if load_spacy_speeches_df:\n",
    "    with open(\"spacy_speeches_df.pkl\", \"rb\") as f:\n",
    "        speeches_df = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers, etc.\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', '', text) # remove text within < >\n",
    "    text = re.sub(r'\\[.*?\\]', '', text) # remove text within [ ]\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text) # remove punctuation\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text) # words with numbers\n",
    "    text = re.sub(r'[‘’“”…]', '', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_df[\"content\"] = speeches_df[\"content\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a language model for spaCy to use\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunking docs 0 to 0\n",
      "chunking docs 0 to 36\n",
      "chunking docs 36 to 72\n",
      "chunking docs 72 to 108\n",
      "chunking docs 108 to 144\n",
      "chunking docs 144 to 180\n",
      "chunking docs 180 to 216\n",
      "chunking docs 216 to 252\n",
      "chunking docs 252 to 288\n",
      "chunking docs 288 to 324\n",
      "chunking docs 324 to 360\n",
      "chunking docs 360 to 396\n",
      "chunking docs 396 to 432\n",
      "chunking docs 432 to 468\n",
      "chunking docs 468 to 504\n",
      "chunking docs 504 to 540\n",
      "chunking docs 540 to 576\n",
      "chunking docs 576 to 612\n",
      "chunking docs 612 to 648\n",
      "chunking docs 648 to 684\n",
      "chunking docs 684 to 720\n",
      "chunking docs 720 to 756\n",
      "chunking docs 756 to 792\n",
      "chunking docs 792 to 828\n",
      "chunking docs 828 to 864\n",
      "chunking docs 864 to 900\n",
      "chunking docs 900 to 936\n",
      "chunking docs 936 to 972\n",
      "chunking docs 972 to 1008\n",
      "chunking docs 1008 to 1044\n"
     ]
    }
   ],
   "source": [
    "# for some reason doing nlp.pipe on all of the documents wouldn't finish, but doing it in chunks does...\n",
    "spacy_docs = []\n",
    "last_idx = 0\n",
    "for idx in np.linspace(0, 1044, 30):\n",
    "    idx_int = int(idx)\n",
    "    print(\"chunking docs {} to {}\".format(last_idx, idx_int))\n",
    "    chunk = list(nlp.pipe(speeches_df.iloc[last_idx:idx_int, 2].values, \n",
    "                          disable=[\"tagger\", \"parser\", \"textcat\"]))\n",
    "    spacy_docs += chunk\n",
    "    last_idx = idx_int\n",
    "    \n",
    "speeches_df[\"spacy_doc\"] = spacy_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy already calculated the lemmatizations for each word, we just need to create new text with just the lemmatizations.\n",
    "\n",
    "We will also remove all named entities from the text at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_lemmatize_remove_ents(doc):\n",
    "    lemmatized_no_ents = []\n",
    "    ents = [e.text for e in doc.ents] # get the list of named entities for this document\n",
    "    for token in doc:\n",
    "        if token.text not in ents: # if this token is not a named entity\n",
    "            lemmatized_no_ents.append(token.lemma_) # add the lemmatized version of the word to the list\n",
    "    lemmatized_no_ents = \" \".join(lemmatized_no_ents) # convert to string\n",
    "    return lemmatized_no_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_no_ents = []\n",
    "for doc in speeches_df[\"spacy_doc\"].values:\n",
    "    lemmatized_no_ents.append(spacy_lemmatize_remove_ents(doc))\n",
    "    \n",
    "speeches_df[\"lemmatized_no_ents\"] = lemmatized_no_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one of President Arthur's speeches was in the wrong century...\n",
    "speeches_df.iloc[765, 1] = np.datetime64(\"1881-12-06\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_spacy_speeches_df = True\n",
    "if save_spacy_speeches_df:\n",
    "    with open(\"spacy_speeches_df.pkl\", \"wb\") as f:\n",
    "        pkl.dump(speeches_df.drop(columns=\"spacy_doc\"), f) # don't save the spacy docs column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
